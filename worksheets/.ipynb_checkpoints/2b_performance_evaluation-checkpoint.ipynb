{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SML-17 Workshop #1\n",
    "## 2- Evaluation of classifiers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week we'll be using the *SPAM base* dataset from the UCI machine learning dataset repository. This dataset comprises a few thousand emails that have been annotated as being spam or not, and the several features have been created from the email text (e.g., presence of certain important words and characters, too many CAPITALS etc.) Note that for many practical applications defining the features is one of the hardest and most important steps. Thankfully this has already been done for us, so we can deploy and evaluate our machine learning algorithms directly. \n",
    "\n",
    "Please see http://archive.ics.uci.edu/ml/datasets/Spambase for a full description of the dataset and feature definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please follow the instructions from the previous lab for importing numpy etc, e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the SPAM data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load the data, which we'll download from\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded (500L, 57L) data points, and (500L,) labels, -1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# download the datafile\n",
    "\n",
    "import sys \n",
    "if sys.version_info[0] >= 3:\n",
    "    from urllib.request import urlretrieve\n",
    "else:\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "#url = \"https://staffwww.dcs.shef.ac.uk/people/T.Cohn/campus_only/mlai13/spambase.data.data\"\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "urlretrieve(url, 'spambase.data.data')\n",
    "\n",
    "\n",
    "# load the CSV file as an array\n",
    "Xandt = np.loadtxt('spambase.data.data', delimiter=',')\n",
    "# randomly shuffle the rows, so as to remove any order bias \n",
    "np.random.shuffle(Xandt)\n",
    "\n",
    "# the last column are the response labels (targets), 0 = not spam, 1 = spam\n",
    "# remap into -1 and +1 and take only the first 500 examples\n",
    "t = Xandt[:500,-1] * 2 - 1\n",
    "# and the remaining columns are the data\n",
    "X = Xandt[:500,:-1]\n",
    "\n",
    "print(\"Loaded\", X.shape, \"data points, and\", t.shape, \"labels,\",  t.min() , t.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has many different types of features, operating on different ranges and with overall very different distributions. Therefore, it is important to standardise each feature. Also, assumptions about the distributions of the input features are central to proofs of the generalisation bounds of several machine learning algorithms.\n",
    "\n",
    "Inspect the minimum, maximum, mean and standard deviation for each column, using `np.mean` and `np.std`. Note that you'll need to supply an axis if you're just interested in columns, e.g., "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.01360000e-01   1.36720000e-01   2.58780000e-01   1.01220000e-01\n",
      "   2.71380000e-01   8.50800000e-02   9.43600000e-02   1.02060000e-01\n",
      "   7.98000000e-02   2.33380000e-01   6.08800000e-02   5.60740000e-01\n",
      "   9.12600000e-02   7.79200000e-02   4.06000000e-02   2.34780000e-01\n",
      "   1.39620000e-01   1.98140000e-01   1.64918000e+00   5.40600000e-02\n",
      "   7.14840000e-01   5.63600000e-02   8.12000000e-02   6.37200000e-02\n",
      "   6.26140000e-01   3.22080000e-01   8.04540000e-01   1.53740000e-01\n",
      "   9.08200000e-02   1.20900000e-01   7.59400000e-02   4.63000000e-02\n",
      "   1.55640000e-01   4.65800000e-02   1.05400000e-01   1.09480000e-01\n",
      "   1.39600000e-01   5.78000000e-03   7.61200000e-02   7.40600000e-02\n",
      "   3.86800000e-02   1.17540000e-01   4.87800000e-02   1.33720000e-01\n",
      "   2.54440000e-01   1.85460000e-01   5.20000000e-03   2.76800000e-02\n",
      "   3.33020000e-02   1.36358000e-01   1.50540000e-02   2.73084000e-01\n",
      "   5.05060000e-02   2.86420000e-02   3.93530600e+00   4.58960000e+01\n",
      "   3.48868000e+02]\n",
      "('Num of features : ', 57)\n"
     ]
    }
   ],
   "source": [
    "tmp = np.mean(X, 0)\n",
    "print(tmp)\n",
    "print('Num of features : ' , len(tmp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the last three features have mean orders of magnitude greater than the other features. (Take a look at their definitions.) Thus, we will standardise all features to have mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = (X - np.mean(X, 0)) / np.std(X, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in previous labs we've plotted the data to get a feeling for how easily the data might be modelled, e.g., if it's linearly separable or requires polynomial basis functions etc. This technique is fine for data with 1 or 2 dimensions, but isn't so straight-forward for 57 dimensional data. (Note that dimensionality reduction methods, such as PCA, can be used to find the most important dimensions for viewing or exploiting in other learning algorithms.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbour classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To mix things up, for this session we will use a k-NN classifier. This classifier is incredibly simple: for each test point find the closest few ($k$) points in the training sets and return the majority label from these points. We will compare the setting of $k$, to try and find the *best* classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining a function *euclidean* to calculate the euclidean distance, $d(\\mathbf{x}, \\mathbf{z}) = \\sqrt{(\\mathbf{x} - \\mathbf{z})^T (\\mathbf{x} - \\mathbf{z})}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean(x, z):\n",
    "    d = x - z\n",
    "    if len(d.shape) > 1 and d.shape[1] > 1:\n",
    "        return np.sqrt(np.diag(np.dot(d, d.T)))\n",
    "    else:\n",
    "        return np.sqrt(np.dot(d, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a function to find nearby training points for a given test point under the euclidean distance. You may want to use `np.argsort` which returns the sort order (as an array indices) for an input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neighbours(x, train_x, k):\n",
    "    dists = euclidean(train_x, x)\n",
    "    return np.argsort(dists)[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this returns the indices of the training points, which can be used to look up the label. Now define the *k-NN* prediction algorithm, which processes each test point and finds the majority class of its neighbours in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn(test_x, train_x, train_t, k):\n",
    "    predict = np.zeros(test_x.shape[0])\n",
    "    for i in range(test_x.shape[0]):\n",
    "        ns = neighbours(train_x, test_x[i], k)\n",
    "        predict[i] = np.sign(np.sum(train_t[ns]))\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heldout evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a large dataset of 500 examples. If we were to use all of these for training, we would have nothing left with which to evaluate the *generalisation error*. Recall that models often *overfit* the training sample, and therefore their performance on this set is often misleading: we have no way of telling if this is due to modelling the true problem or just fitting noise and other idiosyncracies of the training sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate, compute the *training error* of the approach with a few different values of $k$, `[1,3,9,15,33,77]`. We'll later see if other evaluation methods lead to different conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-nn 0.0\n",
      "3-nn 0.09\n",
      "9-nn 0.114\n",
      "15-nn 0.152\n",
      "33-nn 0.17\n",
      "77-nn 0.236\n"
     ]
    }
   ],
   "source": [
    "for k in [1,3,9,15,33,77]:\n",
    "    print '%d-nn' % k, np.sum(knn(X, X, t, k) != t) / float(X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is going on? Which value of k is best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt 1: fixed validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try evaluating on *heldout validation* data. This has been excluded from training, so the model cannot *overfit*. Instead this serves as a fresh data sample, which reflects the intended usage of the classifier in a live scenario (i.e., processing new emails as they arrive in your inbox).\n",
    "\n",
    "One of the easiest methods is to slice the data into two parts. We'll half for training and half for testing. Note that these numbers are fairly arbitrary, but we do want enough test data to get a reliable error estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are (250L, 57L) training samples, and (250L, 57L) heldout test samples\n"
     ]
    }
   ],
   "source": [
    "N = X.shape[0]\n",
    "cut = N/2\n",
    "Xtrain = X[:cut,:]\n",
    "ttrain = t[:cut]\n",
    "Xtest = X[cut:,:]\n",
    "ttest = t[cut:]\n",
    "print \"There are\", Xtrain.shape, \"training samples, and\", Xtest.shape, \"heldout test samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test split: for class label 1:  91 and class label -1 159\n",
      "Train split: for class label 1:  101 and class label -1 149\n"
     ]
    }
   ],
   "source": [
    "print \"Test split: for class label 1: \", np.sum(ttest ==1), \"and class label -1\", np.sum(ttest ==-1)\n",
    "print \"Train split: for class label 1: \", np.sum(ttrain ==1), \"and class label -1\", np.sum(ttrain ==-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your job is to apply $k$-NN on this data for the various values of $k$, and evaluate their training and heldout error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-nn training error 0.0 heldout error 0.196\n",
      "3-nn training error 0.12 heldout error 0.148\n",
      "9-nn training error 0.172 heldout error 0.152\n",
      "15-nn training error 0.204 heldout error 0.184\n",
      "33-nn training error 0.232 heldout error 0.212\n",
      "77-nn training error 0.332 heldout error 0.312\n"
     ]
    }
   ],
   "source": [
    "# IMPLEMENT ME\n",
    "for k in [1,3,9,15,33,77]:\n",
    "    print '%d-nn' % k, \n",
    "    print 'training error', np.sum(knn(Xtrain, Xtrain, ttrain, k) != ttrain) / float(Xtrain.shape[0]),\n",
    "    print 'heldout error', np.sum(knn(Xtest, Xtrain, ttrain, k) != ttest) / float(Xtest.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the heldout error similar to the training error, or do you notice consistent differences?  Can you explain why? What classifier might you select now, and does this match your earlier choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good going. We now have a decent spam classifier, similar to the ones in Google and other email providers (which are trained on *much* more data). You could fairly easily take this approach and the feature definitions and plug this into your own email inbox to classify incoming mails.  Note that scaling this up to the full dataset would require a bit of engineering (or patience!), but otherwise would be straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attempt 2: Leave-one-out cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another validation method is *cross-validation*, which is particularly suitable when you have only a small amount of data. This technique divides the data into parts, called *folds*. Then each fold is used for evaluation, and the model is trained on all other folds. This is repeated, such that we have heldout predictions for the entire dataset.\n",
    "\n",
    "The leave-one-out method is the most extreme version of cross-validation, and defines each fold as a single data-point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement k-NN to work using the leave-one-out method. This only requires a small change from the `knn()` method in order exclude the current point from consideration in the inner loop. Hint: start by taking the $k+1$ nearest neighbours, and then correct the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn_LOO(x, t, k):\n",
    "    predict = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        ns = neighbours(x, x[i], k+1)\n",
    "        if i in ns:\n",
    "            predict[i] = np.sign(np.sum(t[ns]) - t[i])\n",
    "        else:\n",
    "            ns = ns[:k]\n",
    "            predict[i] = np.sign(np.sum(t[ns]))\n",
    "    return predict\n",
    "# Another Implementation by Yasmeen \n",
    "def knn_LOO1(x, t, k):\n",
    "    predict = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        xtrain = np.delete(x, (i), axis=0)\n",
    "        ttrain = np.delete(t, (i), axis=0)\n",
    "        xtest = x[i]\n",
    "        ttest = t[i]\n",
    "        ns = neighbours(xtrain, xtest, k)\n",
    "        #print xtrain.shape , ttrain.shape,xtest.shape, ttest, np.sign(np.sum(t[ns]))\n",
    "        predict[i] = np.sign(np.sum(ttrain[ns]))\n",
    "        \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this out for a few values of k, and compare their error rates. How do these values compare to your results from using a fixed validation set? Are they more reliable, and would you expect the cross-validation error to be higher or lower?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-nn LOO error 0.178\n",
      "3-nn LOO error 0.148\n",
      "9-nn LOO error 0.144\n",
      "15-nn LOO error 0.18\n",
      "33-nn LOO error 0.178\n",
      "77-nn LOO error 0.242\n"
     ]
    }
   ],
   "source": [
    "for k in [1,3,9,15,33,77]:\n",
    "    print '%d-nn' % k, \n",
    "    print 'LOO error', np.sum(knn_LOO1(X, t, k) != t) / float(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-nn LOO error 0.178\n",
      "3-nn LOO error 0.148\n",
      "9-nn LOO error 0.144\n",
      "15-nn LOO error 0.18\n",
      "33-nn LOO error 0.178\n",
      "77-nn LOO error 0.242\n"
     ]
    }
   ],
   "source": [
    "for k in [1,3,9,15,33,77]:\n",
    "    print '%d-nn' % k, \n",
    "    print 'LOO error', np.sum(knn_LOO(X, t, k) != t) / float(X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error types and ROC analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spam classification is an interesting test case. Consider the impact on an email user of various errors:\n",
    "\n",
    "1. misclassifying a spam email as good \n",
    "1. misclassifying a good email as spam \n",
    "\n",
    "We need to balance annoyance (1) with missing potentially important information (2). \n",
    "\n",
    "These two errors are referred to as Type I and Type II errors, and are treated uniformly when measuring accuracy (and 0/1 loss). In this case it is more informative to evaluate the outputs by counting each of the four categories: true positives (true spam which was classified as spam), false positives (good classified as spam), true negatives (good classified as good), and false negatives (spam classified as good)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positives  135\n",
      "false positives 15\n",
      "true negatives  293\n",
      "false negatives 57\n"
     ]
    }
   ],
   "source": [
    "pred = knn_LOO(X, t, 9)\n",
    "print 'true positives ', np.sum(t[pred == 1] == 1)\n",
    "print 'false positives', np.sum(t[pred == 1] == -1)\n",
    "print 'true negatives ', np.sum(t[pred == -1] == -1)\n",
    "print 'false negatives', np.sum(t[pred == -1] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the data are skewed, i.e. there are more negatives than positives. Are the numbers of the two types of error are closely balanced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve gives a better view of the compromise between the two error types. This graph shows the relationship between the *false positive rate* and *true positive rate*. Each point on the curve represents a different classifier. In our case we can vary the value of k, recording the FPR and TPR at each point. Randomly guessing will give us a straight line in ROC space, and we seek to do better by being above and left (higher TPR for a given FPR; lower FPR for a given TPR). For a thorough description please read the wikipedia page on ROC analysis http://en.wikipedia.org/wiki/Receiver_operating_characteristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7d19b38>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEPCAYAAACzwehFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7b941d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "for k in [1,2,3,5,9,15,33,77]:\n",
    "    pred = knn_LOO(X, t, k)\n",
    "    \n",
    "    tp = np.sum(t[pred==1] == 1)\n",
    "    fp = np.sum(t[pred==1] == -1)\n",
    "    tn = np.sum(t[pred==-1] == -1)\n",
    "    fn = np.sum(t[pred==-1] == 1)\n",
    "    \n",
    "    # now compute the TPR and FPR\n",
    "    fpr = fp / float(fp + tn)\n",
    "    tpr = tp / float(tp + fn)\n",
    "    \n",
    "    \n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "\n",
    "plot(fprs, tprs, 'r*')\n",
    "xlabel('FPR')\n",
    "ylabel('TPR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on our appetite for false positive errors, we can read off the corresponding accuracy for predictions (TPR) with respect to false negatives. Plotting the ROC curve for several classifiers can help us choose which is best while incorporating unbalanced loss functions."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
