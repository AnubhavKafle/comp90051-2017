{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop we are going to implement a simple feed-forward artificial neural network (ANN) from scratch. The aim of this exercise is to give you a deeper understanding of how ANNs work \"under the hood\". Moreover, in this workshop, we are going to train an ANN for a classification task. Note that we do not have to come up with a particularly efficient ANN implementation (i.e., the one that works fast on large datasets). Rather our priority is to *develop code that works*, and is *easy to understand*. Also this exercise is not all about coding, but includes doing some simple maths too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with generating the data for binary classification. We are going to re-use a slightly modified dataset generation funciton from last week. The difference is that now the two classes are encoded as $0$ and $1$. In addition, we are not going to use collections any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_s_shaped_data(gap=3):\n",
    "    X = np.random.randn(80, 2)\n",
    "\n",
    "    X[10:20] += np.array([3, 4])\n",
    "    X[20:30] += np.array([0, 8])\n",
    "    X[30:40] += np.array([3, 12])\n",
    "\n",
    "    X[40:50] += np.array([gap, 0])\n",
    "    X[50:60] += np.array([3 + gap, 4])\n",
    "    X[60:70] += np.array([gap, 8])\n",
    "    X[70:80] += np.array([3 + gap, 12])\n",
    "\n",
    "    y = np.hstack([np.zeros(40), np.ones(40)])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this function to generate data that is in general separable, but requires a non-linear separation boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = generate_s_shaped_data(5)\n",
    "plt.plot(X[y==0,0], X[y==0,1], \"o\")\n",
    "plt.plot(X[y==1,0], X[y==1,1], \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a particular ANN configuration that we are going to implement (see the figure below). We are working with two-dimensional input, hence two input units. Furthermore, we are going to do binary classification, for which one output unit would be sufficient. However, just to try backpropagation with multidimensional output, we are going to use two output units and their values will be indicative of conditional probabilities of each class $P(y=class_i|\\bf{x},\\bf{v},\\bf{w})$. Finally, the number of hidden units $p$ will be provided by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.andreykan.com/stuff/workshop5-ann.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equations define how the model is computed. Here $\\sigma$ denotes logistic function. The derivatives are not used during model computation (forward computation), but we are going to need the soon during training. We are going to implement this ANN. Note that you can use *tanh* funciton from numpy, but we need to implement the logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(s):\n",
    "    # ... your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's implement the forward computation for a single instance given some parameter values. Note that this function returns the output layer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x is a 2 element input representing a single training instance;\n",
    "#\n",
    "# V is a matrix with 3 rows and p columns, where the three rows\n",
    "#     correspond to the bias and weights for the two inputs,\n",
    "#     and columns correspond to hidden units;\n",
    "#\n",
    "# W is a matrix with (p+1) rows and 2 columns, where the rows\n",
    "#     correspond to the bias and p hidden units, and columns\n",
    "#     correspond to output elements;\n",
    "#\n",
    "# returns: a two element output layer, and a vector of hidden\n",
    "#          node values with (p+1) elements, where the first\n",
    "#          element is constant 1\n",
    "#\n",
    "def compute_forward(x,V,W):\n",
    "\n",
    "    # append input, so that the bias can be handled naturally\n",
    "    x_ext = np.append(1, x)\n",
    "\n",
    "    # get the number of hidden units\n",
    "    p = V.shape[1]\n",
    "    u = np.zeros((p))\n",
    "\n",
    "    # iterate over hidden units\n",
    "    for i in range(p):\n",
    "        \n",
    "        u[i] = # ... your code here ...\n",
    "\n",
    "    # append hidden layer, so that the bias can be handled naturally\n",
    "    u_ext = np.append(1, u)\n",
    "\n",
    "    # set the outputs\n",
    "    z = np.zeros((2))\n",
    "    \n",
    "    z[0] = # ... over to you ...\n",
    "    z[1] = # ... over to you ...\n",
    "\n",
    "    return z, u_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's implement a function that makes predictions based on the output layer values. This function is going to make predictions for the entire dataset. After implementing these two functions, you might like to play with toy data and manually picked parameter values just to validate the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X is a matrix with N rows and 2 columns, where\n",
    "#     rows represent training instances\n",
    "#\n",
    "# V and W have the same interpretation as in compute_forward()\n",
    "#\n",
    "# returns: an N element vector with predictions (0 or 1 each)\n",
    "#\n",
    "def ann_predict(X,V,W):\n",
    "    num_examples = X.shape[0]\n",
    "    y_pred = np.zeros(num_examples)\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        y_pred[i] = # ... over to you\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model, we need to develop a training algorithm. Recall that the idea is to define a loss function and then find parameter values that minimise the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training example comes with a true label which is either $0$ or $1$. For convenicence, we are going to encode the label as a two-component vector $\\bf{y}$, such that only one of the components is one and another one is zero. Moreover, we will make a simplifying assumption that the two components are independent to get $$P\\left(\\bf{y}|\\bf{x},\\bf{V},\\bf{W}\\right)=\\prod\\limits_{k=1,2}z_k\\left(\\bf{x},\\bf{V},\\bf{W}\\right)^{y_k}\\left(1 - z_k\\left(\\bf{x},\\bf{V},\\bf{W}\\right)\\right)^{1-y_k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly a wrong assumption, but it is going to be good enough for us to get an ANN working. This assumption can be dropped by using an additional special layer called *soft-max layer*, but this is beyond the scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the algorithm of this probability and inverting the sign, so that maximising probability leads to minimising the loss, gives us cross-entropy loss (for a single training example) $$l\\left(\\bf{V},\\bf{W}\\right)=-\\sum\\limits_{k=1,2}y_kln(z_k)+(1-y_k)ln\\left(1 - z_k\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that computes the loss for a single training example and true label encoded as vector $\\bf{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x, V and W have the same interpretation as in compute_forward()\n",
    "#\n",
    "# y is a two element encoding of a binary label, either t[0] = 1\n",
    "#     and t[1] = 0, or the other way around\n",
    "#\n",
    "# returns: loss for a given training example and parameters\n",
    "#\n",
    "def compute_loss(x,y,V,W):\n",
    "    # ... your code here ...\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use stochastic gradient descent, and in each iteration of this algorithm we need to compute parameter updates. The updates are based on partial derivatives $\\frac{\\partial l}{\\partial v_{ij}}$ and $\\frac{\\partial l}{\\partial w_{jk}}$. We are going to compute these derivatives using auxiliary quantities $\\delta_k$ and $\\varepsilon_{jk}$. Note that the multidimensional output, $\\varepsilon_{jk}$, has two indices. Also note that the equations below assume that $x$ is a three-dimensional vector, after appending with a constant one to capture the bias, and, similarly, that $u$ is a $(p+1)$-dimensional vector with the first element constant one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $l_k=-y_kln(z_k)-(1-y_k)ln\\left(1 - z_k\\right)$. The auxiliary quantities are $\\delta_k=\\frac{\\partial l}{\\partial s_k}$ and $\\varepsilon_{jk}=\\frac{\\partial l_k}{\\partial r_j}$. Use the identities provided in the ANN figure above to verify that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\delta_k=\\frac{\\partial l_k}{\\partial z_k}\\frac{\\partial z_k}{\\partial s_k}=(z_k-y_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial l}{\\partial w_{jk}}=\\delta_ku_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\varepsilon_{jk}=\\frac{\\partial l_k}{\\partial z_k}\\frac{\\partial z_k}{\\partial s_k}\\frac{\\partial s_k}{\\partial u_j}\\frac{\\partial u_j}{\\partial r_j}=\\delta_k(1-u^2_j)w_{jk}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial l}{\\partial v_{ij}}=\\frac{\\partial l_1}{\\partial v_{ij}}+\\frac{\\partial l_2}{\\partial v_{ij}}=\\varepsilon_{j1}x_i+\\varepsilon_{j2}x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use these equations to implement a single update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x, V and W have the same interpretation as in compute_forward()\n",
    "#\n",
    "# y has the same interpretation as in compute_loss()\n",
    "#\n",
    "# returns: updated estimates of V and W\n",
    "#\n",
    "def update_params(x,y,V,W,eta):\n",
    "    ### forward computation\n",
    "    z, u_ext = compute_forward(x,V,W)\n",
    "\n",
    "    # ... over to you\n",
    "    ## use backpropagation equations to compute dV and dW\n",
    "\n",
    "    V += -eta*dV\n",
    "    W += -eta*dW\n",
    "\n",
    "    return V,W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally use the single update step in a function that performs training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X is a matrix with N rows and 2 columns, where\n",
    "#     rows represent training instances\n",
    "#\n",
    "# V0 and W0 are starting parameter values\n",
    "# as before, V0 is a matrix with 3 rows and p columns, where\n",
    "#     the three rows correspond to the bias and weights for\n",
    "#     the two inputs, and columns correspond to hidden units;\n",
    "#\n",
    "# W0 is a matrix with (p+1) rows and 2 columns, where the rows\n",
    "#     correspond to the bias and p hidden units, and columns\n",
    "#     correspond to output elements;\n",
    "#\n",
    "# y is an N element array of true labels\n",
    "#\n",
    "# returns: trained values for V and W, as well as total loss\n",
    "#          after each training epoch\n",
    "#\n",
    "def ann_train(X,y,V0,W0):\n",
    "    # use starting values\n",
    "    V = V0\n",
    "    W = W0\n",
    "\n",
    "    # step scale; note that this is usually changed (decreased)\n",
    "    # between iterations, but we won't bother here\n",
    "    eta = 0.01\n",
    "\n",
    "    # number of rounds over the data\n",
    "    num_epochs = 50\n",
    "\n",
    "    # number of training examples\n",
    "    num_examples = X.shape[0]\n",
    "\n",
    "    # calculate total loss in each epoch\n",
    "    l_total = np.zeros(num_epochs)\n",
    "\n",
    "    # make several rounds over the data\n",
    "    for j in range(num_epochs):\n",
    "        # iterate trough each training example\n",
    "        l_total[j] = 0\n",
    "        for i in range(num_examples):\n",
    "            curr_x = X[i,:]\n",
    "            curr_y = np.zeros(2)\n",
    "            curr_y[0] = (y[i] == 0)\n",
    "            curr_y[1] = 1 - curr_y[0]\n",
    "            V,W = update_params(curr_x,curr_y,V,W,eta)\n",
    "            l_total[j] += compute_loss(curr_x,curr_y,V,W)\n",
    "\n",
    "    return V,W,l_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try everything in action! We will start from some randomly generated parameters, perform training and compare the accuracy before and after the training. *Why not start with all parameters equal to zero?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of hidden units\n",
    "p = 5\n",
    "\n",
    "# initialisation\n",
    "V0 = np.random.randn(3, p)\n",
    "W0 = np.random.randn(p + 1, 2)\n",
    "\n",
    "y_pred = ann_predict(X,V0,W0)\n",
    "print('Proportion misclassified:')\n",
    "prop = 1 - np.sum(y_pred == y) / float(y.shape[0])\n",
    "if prop > 0.5:\n",
    "    prop = 1 - prop\n",
    "print(prop)\n",
    "\n",
    "\n",
    "V,W,l_total = ann_train(X,y,V0,W0)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(l_total.shape[0]), l_total, '.-')\n",
    "\n",
    "y_pred = ann_predict(X,V,W)\n",
    "print('Proportion misclassified:')\n",
    "prop = 1 - np.sum(y_pred == y) / float(y.shape[0])\n",
    "if prop > 0.5:\n",
    "    prop = 1 - prop\n",
    "print(prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our training procedure is not regularised. A natural next step would be to introduce regularisation weighted by $\\lambda$, and estimate training meta-parameters, such as $p$, $\\eta$, *number of epochs* and $\\lambda$ using heldout validation. Another direction for future work is extending this ANN for multi-label classifictation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
